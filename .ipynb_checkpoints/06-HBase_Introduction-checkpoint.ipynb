{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Goal\n",
    "This training is going to cover basic HBase usage (more on operation)\n",
    "1. Use HBase Shell to check available table\n",
    "2. PUT/GET/SCAN\n",
    "\n",
    "- Connection Method and Data Query\n",
    "    - sample code for python\n",
    "    - sample code for scala\n",
    "\n",
    "Detailed Training will be covered by Bernard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBase Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/HBase.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basic Concept\n",
    "rowkey\n",
    "column\n",
    "timestamp\n",
    "cell\n",
    "value\n",
    "\n",
    "lexicographically order\n",
    "\n",
    "Order 1, 134, 12, 43, 54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBase Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list \"abc.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hbase> get ‘t1’, ‘r1’\n",
    "hbase> get ‘t1’, ‘r1’, {TIMERANGE => [ts1, ts2]}\n",
    "hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’}\n",
    "hbase> get ‘t1’, ‘r1’, {COLUMN => [‘c1’, ‘c2’, ‘c3’]}\n",
    "hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’, TIMESTAMP => ts1}\n",
    "hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’, TIMERANGE => [ts1, ts2], VERSIONS => 4}\n",
    "hbase> get ‘t1’, ‘r1’, {COLUMN => ‘c1’, TIMESTAMP => ts1, VERSIONS => 4}\n",
    "hbase> get ‘t1’, ‘r1’, {FILTER => “ValueFilter(=, ‘binary:abc’)”}\n",
    "hbase> get ‘t1’, ‘r1’, ‘c1’\n",
    "hbase> get ‘t1’, ‘r1’, ‘c1’, ‘c2’\n",
    "hbase> get ‘t1’, ‘r1’, [‘c1’, ‘c2’]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hbase> scan ‘.META.’, {COLUMNS => ‘info:regioninfo’}\n",
    "hbase> scan ‘t1’, {COLUMNS => [‘c1’, ‘c2’], LIMIT => 10, STARTROW => ‘xyz’}\n",
    "hbase> scan ‘t1’, {COLUMNS => ‘c1’, TIMERANGE => [1303668804, 1303668904]}\n",
    "hbase> scan ‘t1’, {FILTER => “(PrefixFilter (‘row2’) AND\n",
    "(QualifierFilter (>=, ‘binary:xyz’))) AND (TimestampsFilter ( 123, 456))”}\n",
    "hbase> scan ‘t1’, {FILTER =>\n",
    "org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python (support both python 2 and 3)\n",
    "#### Source Code: \n",
    "##### Make sure the thrift server is enabled (production cluster and streaming cluster is enabled)\n",
    "\n",
    "#### Key Library: mu_hbasethrift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import mu_hbasethrift\n",
    "import pandas as pd\n",
    "\n",
    "def hbase_query(tablename):\n",
    "    host = 'fslhdpiname03'\n",
    "    port = 9090\n",
    "    connection = mu_hbasethrift.Connection(host, port)\n",
    "    connection.open()\n",
    "    table = connection.table(tablename)\n",
    "    return table\n",
    "\n",
    "def rows_rename_lot(rows, col_dict):\n",
    "    rows_decode_rename = []\n",
    "    for row in rows:\n",
    "        rowkey, row = row\n",
    "        lot_id, mfg_process_step = decouple_rowkey_lot(rowkey.decode())\n",
    "        rowkey_dict = {\n",
    "            'lot_id': str(lot_id)[0:6][::-1] + str(lot_id)[6:],\n",
    "            'mfg_process_step': str(mfg_process_step)\n",
    "        }\n",
    "        rows_decode_rename.append({**row_rename(row, col_dict), **rowkey_dict})\n",
    "    return rows_decode_rename\n",
    "\n",
    "def decouple_rowkey_lot(rowkey):\n",
    "    rowkey_list = rowkey.split(\"_\")\n",
    "    lot_id = rowkey_list[0]\n",
    "    mfg_process_step = rowkey_list[1]\n",
    "    return lot_id, mfg_process_step\n",
    "\n",
    "def row_rename(row, col_dict):\n",
    "    row_decode = {key.decode(): val.decode() for key, val in row.items()}\n",
    "    row_decode_rename = dict((col_dict[key], value) for (key, value) in row_decode.items())\n",
    "    return row_decode_rename\n",
    "\n",
    "def filter_steps(self, df):\n",
    "    selected_step_list = df['mfg_process_step']\n",
    "    selected_step_flag = [x[0] in ('1', '3', '4', '5', '6', '8') for x in selected_step_list]\n",
    "    return selected_step_flag\n",
    "\n",
    "def reverse_lot_id(lot_id):\n",
    "    return str(lot_id)[0:6][::-1] + str(lot_id)[6:]\n",
    "\n",
    "def hbase_query_sigma_lot(lot_id, sigma_lot_table_name):\n",
    "    table_sigma_lot = hbase_query(sigma_lot_table_name)\n",
    "\n",
    "    columns = {\n",
    "    'cf:TRAVELER_ID': 'traveler_id',\n",
    "    'cf:EQUIPMENT_ID': 'equipment_id',\n",
    "    'cf:PROCESS_ID': 'process_id'}\n",
    "    \n",
    "    rowkey = reverse_lot_id(lot_id)\n",
    "    rows = table_sigma_lot.scan(row_prefix=rowkey, include_timestamp=False, columns=list(columns.keys()))\n",
    "    results = rows_rename_lot(rows, columns)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    if results_df.shape[0] > 0:\n",
    "        hbase_query_close(table_sigma_lot)\n",
    "        return results_df\n",
    "    else:\n",
    "        hbase_query_close(table_sigma_lot)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def hbase_query_close(table):\n",
    "    connection = table.connection\n",
    "    connection.close()\n",
    "\n",
    "SIGMA_LOT_CONFIG = {\n",
    "    'cf:TRAVELER_ID': 'traveler_id',\n",
    "    'cf:EQUIPMENT_ID': 'equipment_id',\n",
    "    'cf:PROCESS_ID': 'process_id'}\n",
    "sigma_lot_table_name = \"prod_mti_singapore_fab_10_sigma:sigma_lot\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scala\n",
    "```scala\n",
    "\n",
    "import org.apache.hadoop.fs.{LocatedFileStatus, Path, RemoteIterator}\n",
    "import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory, Scan, Get,Put}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName}\n",
    "import java.time.Instant\n",
    "import org.apache.spark.sql.Row\n",
    "import java.util.Calendar \n",
    "import java.text.SimpleDateFormat\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.hadoop.hbase.filter._\n",
    "\n",
    "object SharedHBaseConnection extends Serializable{\n",
    "  private var sharedConnection: Option[Connection] = None\n",
    "\n",
    "  lazy val config = {\n",
    "    val config = HBaseConfiguration.create()\n",
    "    config.addResource(new Path(\"file:///usr/hdp/current/hbase-client/conf/hbase-site.xml\"))\n",
    "    config\n",
    "  }\n",
    "\n",
    "  def apply(): Connection = synchronized {\n",
    "    sharedConnection.getOrElse {\n",
    "      val connection = ConnectionFactory.createConnection(config)\n",
    "      sharedConnection = Some(connection)\n",
    "      connection\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "//Define Time Rnage\n",
    "val starttime=1522817592071L\n",
    "val endtime=1522817592073L\n",
    "\n",
    "\n",
    "val hbaseConnection = SharedHBaseConnection()\n",
    "val dataTable = hbaseConnection.getTable(TableName.valueOf(\"prod_mti_singapore_fab_10_sigma:sigma_lot\"))\n",
    "val dataScan = new Scan()\n",
    "\n",
    "//Add Multi Row Range Key \n",
    "\n",
    "//Add Signal Filter\n",
    "val filter = new ColumnRangeFilter(Bytes.toBytes(starttime), true, Bytes.toBytes(endtime), false);\t\t\t\t\t\t\t \n",
    "\t\t\t\t\t\t\t\t \n",
    "dataScan.setFilter(filter);\n",
    "dataScan.setCaching(5000)\n",
    "dataScan.setMaxVersions(1)\n",
    "dataScan.setTimeRange (starttime, endtime)\n",
    "\n",
    "val startkey=Bytes.toBytes(\"0000091_0001-01\")\n",
    "val endkey=Bytes.toBytes(\"0000091_0001-25\")\n",
    "val cf=Bytes.toBytes(\"cf\")\n",
    "dataScan.addFamily(cf)\n",
    "dataScan.setStartRow(startkey)\n",
    "dataScan.setStopRow(endkey)\n",
    "val calendar_start = Calendar.getInstance().getTime().getTime()\n",
    "val scanresult =dataTable.getScanner(dataScan)\n",
    "val iter=scanresult.iterator\n",
    "\n",
    "val bodymessage=zero_count(result_count)\n",
    "val w = new BufferedWriter(new FileWriter(\"/home/hdfsf10n/apps/test/zero_count.txt\"))\n",
    "w.write(bodymessage)\n",
    "w.close()\n",
    "hbaseConnection.close\n",
    "\n",
    "exit()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter \n",
    "\n",
    "https://www.cloudera.com/documentation/enterprise/5-3-x/topics/admin_hbase_filtering.html\n",
    "\n",
    "Bloom Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KeyOnlyFilter - takes no arguments. Returns the key portion of each key-value pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FirstKeyOnlyFilter - takes no arguments. Returns the key portion of the first key-value pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PrefixFilter - takes a single argument, a prefix of a row key. It returns only those key-values present in a row that start with the specified row prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ColumnPrefixFilter - takes a single argument, a column prefix. It returns only those key-values present in a column that starts with the specified column prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MultipleColumnPrefixFilter - takes a list of column prefixes. It returns key-values that are present in a column that starts with any of the specified column prefixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ColumnCountGetFilter - takes one argument, a limit. It returns the first limit number of columns in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PageFilter - takes one argument, a page size. It returns page size number of rows from the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ColumnPaginationFilter - takes two arguments, a limit and offset. It returns limit number of columns after offset number of columns. It does this for all the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- InclusiveStopFilter - takes one argument, a row key on which to stop scanning. It returns all key-values present in rows up to and including the specified row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TimeStampsFilter - takes a list of timestamps. It returns those key-values whose timestamps matches any of the specified timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RowFilter - takes a compare operator and a comparator. It compares each row key with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FamilyFilter - takes a compare operator and a comparator. It compares each family name with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QualifierFilter - takes a compare operator and a comparator. It compares each qualifier name with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ValueFilter - takes a compare operator and a comparator. It compares each value with the comparator using the compare operator and if the comparison returns true, it returns that key-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DependentColumnFilter - takes two arguments required arguments, a family and a qualifier. It tries to locate this column in each row and returns all key-values in that row that have the same timestamp. If the row does not contain the specified column, none of the key-values in that row will be returned.\n",
    "The filter can also take an optional boolean argument, dropDependentColumn. If set to true, the column used for the filter does not get returned.\n",
    "\n",
    "The filter can also take two more additional optional arguments, a compare operator and a value comparator, which are further checks in addition to the family and qualifier. If the dependent column is found, its value should also pass the value check. If it does pass the value check, only then is its timestamp taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SingleColumnValueFilter - takes a column family, a qualifier, a compare operator and a comparator. If the specified column is not found, all the columns of that row will be emitted. If the column is found and the comparison with the comparator returns true, all the columns of the row will be emitted. If the condition fails, the row will not be emitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SingleColumnValueExcludeFilter - takes the same arguments and behaves same as SingleColumnValueFilter. However, if the column is found and the condition passes, all the columns of the row will be emitted except for the tested column value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ColumnRangeFilter - takes either minColumn, maxColumn, or both. Returns only those keys with columns that are between minColumn and maxColumn. It also takes two boolean variables to indicate whether to include the minColumn and maxColumn or not. If you don’t want to set the minColumn or the maxColumn, you can pass in an empty argument."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
